# 토큰(Token)

## 토큰이란?
토큰은 자연어 처리(NLP, Natural language processing)에서 의미를 가진 최소 단위로  단어, 문자, 단어의 일부가 될 수 있음  
언어 모델이 텍스트를 처리할 때는 토큰 단위로 변환하여 처리

## 토큰의 주요 종류

### 1. 문자 토큰화 (Character Tokenization)
- 텍스트를 개별 문자로 분할
- 예: "안녕" → ["안", "녕"]
- 장점: 어휘 크기가 작고, 미등록 문자 문제 없음
- 단점: 의미적 정보가 손실될 수 있고, 시퀀스 길이가 길어짐
 
### 2. 단어 토큰화 (Word Tokenization)
- 텍스트를 개별 단어로 분할, 일반적으로 공백이나 구두점을 기준으로 나눔
- 예: "나는 학교에 갑니다." → ["나는", "학교에", "갑니다."]
- 장점: 직관적이고 이해하기 쉬움
- 단점: 어휘 크기가 매우 커질 수 있고, 미등록 단어 문제가 발생

### 3. 하위 단어 토큰화 (Subword Tokenization)
- 단어를 더 작은 의미 단위로 분할
- 자주 등장하는 단어는 그대로 유지하고, 드문 단어는 더 작은 단위로 분할
- 예: "자연어처리" → ["자연", "어", "처리"] 또는 ["자연어", "처리"]
- 장점: 어휘 크기를 효율적으로 관리하면서도 의미 정보를 보존함
- 단점: 토큰화 과정이 더 복잡함

## 토큰화의 중요성
1. **모델 크기와 성능**:   
   토큰화 방식은 모델의 어휘 크기와 직접적인 관련이 있으며, 이는 모델 크기와 학습 효율성에 영향을 미침
2. **언어적 특성 반영**:   
   언어마다 단어 구성 방식이 다르므로, 토큰화 방식에 따라 언어적 특성을 얼마나 잘 반영할 수 있는지가 달라짐
3. **Out-of-Vocabulary(OOV, 미등록 단어) 처리**:   
   하위 단어 토큰화는 학습 데이터에 없던 새로운 단어도 기존 하위 단어의 조합으로 표현할 수 있어 OOV 문제를 완화힘

언어 모델(예: GPT, BERT)에서는 대부분 하위 단어 토큰화 방식을 사용하여 어휘 크기를 효율적으로 관리하면서도 다양한 단어를 표현