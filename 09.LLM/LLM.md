# LLM(Large Language Model)

---

## 1. LLM의 개념

### LLM이란?
- 대규모 텍스트 데이터로 학습된 신경망 기반 언어 모델로 텍스트 생성, 이해, 번역 등 다양한 자연어 처리 작업 수행 
- 수십억에서 수천억 개의 매개변수를 가진 모델들은 인간과 유사한 텍스트를 생성하고 추론 할 수 있는 능력 보유

### LLM의 작동 원리

#### 1. 토큰화(Tokenization)
텍스트를 모델이 처리할 수 있는 토큰 단위로 분할
```
"인공지능은 미래다" → ["인공", "지능", "은", " 미래", "다"]
```
#### 2. 임베딩(Embedding)
각 토큰을 고차원 벡터로 변환

#### 3. 트랜스포머 레이어 처리
- **자기 주의 메커니즘(Self-Attention)**: 문장의 각 단어가 다른 모든 단어와 얼마나 관련이 있는지 계산
- **피드포워드 네트워크(Feed-Forward Network)**: 정보를 가공해서 의미 있게 바꿔주는 필터 역할, 표현 변환
- **다층 구조**: 여러 레이어를 통과하며 처음에는 단어 뜻만 알지만, 점점 문장의 전체적인 의미까지 파악하여 정보 처리

#### 4. 다음 토큰 예측
현재까지의 컨텍스트를 기반으로 다음에 올 가능성이 높은 토큰 예측

---

## 2. LLM의 역할

### LLM의 핵심 기능

#### 1. 정보 통합 및 요약
- 사용자의 의도를 이해하고, 검색된 여러 문서의 정보를 일관된 응답으로 통합
- 중복 정보 제거 및 핵심 정보 강조
- 사용자 질의에 맞게 관련 정보 요약

#### 2. 추론 및 분석
- 검색된 정보를 바탕으로 논리적 추론 수행
- 암시적 정보 파악 및 연결
- 여러 정보 조각 간의 관계 분석

#### 3. 응답 생성
- 사용자 질의에 문맥을 고려하여 적합한 형식과 톤으로 응답 생성
- 검색된 정보와 기존 지식 조합
- 프롬프트의 지시사항 준수

#### 4. 불확실성 처리
- 정보 부족 시 적절한 불확실성 표현
- 상충되는 정보 식별 및 처리
- 출처 인용 및 신뢰도 평가

### RAG 장점
- **최신 정보 접근**: 학습 데이터 이후의 최신 정보 활용 가능
- **사실 기반 응답**: 환각(hallucination) 현상 감소
- **투명성 향상**: 정보 출처 제공 가능
- **도메인 특화**: 특정 분야 문서로 전문성 강화
- **확장성**: 새로운 지식을 쉽게 추가 가능

---

## 3. 주요 LLM 모델 비교 및 선택 가이드

### 상용 LLM

#### OpenAI 모델
- **GPT-4/GPT-4o**: 최고 수준의 추론 능력, 멀티모달 지원, 높은 정확도
- **GPT-3.5 Turbo**: 비용 효율적, 빠른 응답 속도, 기본적인 RAG 작업에 충분
- **특징**: 강력한 API, 지속적 업데이트, 높은 안정성
- **비용**: 토큰 기반 요금, 고급 모델일수록 높은 비용

#### Anthropic 모델
- **Claude 3 Opus**: 최고 수준의 추론 및 분석 능력, 긴 컨텍스트 지원
- **Claude 3 Sonnet/Haiku**: 다양한 성능과 비용 옵션
- **특징**: 안전성 강조, 긴 컨텍스트 처리 우수(최대 200K+ 토큰)
- **비용**: OpenAI와 유사한 토큰 기반 요금 체계

#### 기타 상용 모델
- **Google Gemini**: 멀티모달 강점, 구글 서비스 통합
- **Cohere Command**: 기업용 특화, 데이터 보안 강조
- **AI21 Jurassic**: 긴 문서 처리 및 생성 특화

### 오픈소스 LLM

#### Meta 모델
- **LLaMA 2/3**: 고성능 오픈소스 기반, 다양한 크기(7B~70B)
- **특징**: 상업적 사용 허용, 활발한 생태계, 광범위한 미세조정 모델

#### Mistral AI 모델
- **Mistral 7B**: 작은 크기 대비 우수한 성능
- **Mixtral 8x7B**: MoE 구조, 더 큰 모델에 준하는 성능
- **특징**: 효율적인 아키텍처, 오픈 라이선스

#### 한국어 오픈소스 모델
- **Exaone**: [LG AI Research 제공 오픈 소스](https://huggingface.co/LGAI-EXAONE)
- **Solar Pro**: [Upstage사 제공 사용 모델](https://www.upstage.ai/blog/en/solar-pro)

---

## 4. LLM Hyper Parameter

### 주요 Hyper Parameter

#### 1. Temperature
- 텍스트의 생성의 무작위성/창의성 조절 (0.0 ~ 0.3 설정, 사실적 정보 위주)
- 낮은 Temperature: 사실적, 반복적 응답,  높은 Temperature: 창의적 응답

#### 2. Top-p (핵 샘플링)
- 응답 후보 토큰의 확률 분포 조절(0.0(0%) 항상 가장 높은 확률 토큰, 0.1 ~ 0.3 설정), 랜덤하게 출력
- 낮은 Top-p: 사실적 정보 위주, 높은 Top-p(0.9) : 다양한 답변 생성(후보 단어 90% 중에서 생성) 
- Top-p : 0.1 처럼 낮은 값인 경우, 가장 가능성이 높은 토큰에서만 선택

#### 3.Top-k ( 상위 k)
- 응답을 위해 고려할 후보 토큰 개수(RAG 40~50개 선택적 사용)

#### 4. Max Tokens(최대 토큰)
- 생성되는  답변 텍스트의 최대 길이(모델별 상이, 64 ~ 수만 토큰)

#### 5. Frequency Penalty(빈도 페널티)
- 해당 토큰이 응답 및 프롬프트에 등장한 빈도에 비례하여 다음에 등장할 토큰에 불이익을 적용(반복 단어/구문 억제)
- Frequency Penalty가 높을수록 단어가 다시 등장할 가능성이 줄어듦

#### 6. Presence Penalty(존재 페널티)
- 이미 등장(생성)한 모든 토큰에 동일한 페널티가 적용(이미 등장한 주제 재사용 억제)
- 다양하거나 창의적인 텍스트를 생성하기 위해 더 높은 Presence Penalty 사용

---

## 참고 자료
- [실습코드] : [LangChain 한국어 튜토리얼](https://github.com/teddylee777/langchain-kr)